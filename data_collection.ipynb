{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1b294b",
   "metadata": {},
   "source": [
    "# Data collection - Air pollution\n",
    "Code in this file is used to extract historical data for PM10 and PM2.5 for chosen station from GIO≈ö API. The file is divide into parts:\n",
    "- Importing dependencies\n",
    "- Defining parameters\n",
    "- Functions\n",
    "- Data extracts\n",
    "- Saving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d554c",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7a6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # HTTP requests\n",
    "import pandas as pd # Data manipulation\n",
    "from datetime import datetime, timedelta # Date and time manipulation\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbb685",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eae759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = 731  # Gda≈Ñsk, ul. Wyzwolenia\n",
    "start_date = datetime(2025, 1, 1) # Start date for data collection\n",
    "end_date = datetime(2025, 6, 30) # End date for data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c80823",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd7070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensors(station_id):\n",
    "    \"\"\"\n",
    "    Function to get the list of sensors for a given station ID from GIO≈ö API.\n",
    "    Args:\n",
    "        station_id (int): The ID of the station to get sensors for.\n",
    "    Returns:\n",
    "        list: A list of sensors for the specified station, or an empty list if the format is incorrect.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.gios.gov.pl/pjp-api/v1/rest/station/sensors/{station_id}\" # API URL \n",
    "    resp = requests.get(url) # GET request to the API\n",
    "    resp.raise_for_status() # Raise an error for bad responses\n",
    "    data = resp.json() # JSON response\n",
    "    sensors = data.get(\"Lista stanowisk pomiarowych dla podanej stacji\", []) # Extracting the list of sensors\n",
    "\n",
    "    if isinstance(sensors, list) and all(isinstance(s, dict) for s in sensors): # Check if the response is a list of dictionaries\n",
    "        return sensors\n",
    "    else: # If the response is not in the expected format, print an error message\n",
    "        print(\"Incorrect format\", sensors)\n",
    "        return []\n",
    "\n",
    "def get_archival_data(sensor_id, date_from_str, date_to_str):\n",
    "    \"\"\"\n",
    "    Function to get archival data for a specific sensor from GIO≈ö API.\n",
    "    Args:\n",
    "        sensor_id (int): The ID of the sensor to get data for.\n",
    "        date_from_str (str): Start date in 'YYYY-MM-DD' format.\n",
    "        date_to_str (str): End date in 'YYYY-MM-DD' format.\n",
    "    Returns:\n",
    "        list: A list of measurements for the specified sensor and date range.\n",
    "    \"\"\"\n",
    "    base_url = f\"https://api.gios.gov.pl/pjp-api/v1/rest/archivalData/getDataBySensor/{sensor_id}\"\n",
    "    params = {\n",
    "        \"dateFrom\": date_from_str,\n",
    "        \"dateTo\": date_to_str,\n",
    "        \"page\": 0, # Start from the first page\n",
    "        \"size\": 100 # Number of records per page\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "    while True: # Loop to handle pagination\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=params)\n",
    "            if resp.status_code == 429: # Rate limit exceeded\n",
    "                print(\"Paused due to rate limit. Retrying in 2 seconds...\")\n",
    "                time.sleep(2) # Modify the sleep time as needed\n",
    "                continue\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            measurements = data.get(\"Lista archiwalnych wynik√≥w pomiar√≥w\", [])\n",
    "            all_data.extend(measurements)\n",
    "\n",
    "            total_pages = data.get(\"totalPages\", 1)\n",
    "            if params[\"page\"] >= total_pages - 1:\n",
    "                break\n",
    "\n",
    "            params[\"page\"] += 1\n",
    "            time.sleep(1.2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with sensor: {sensor_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def process_hourly_to_daily(data):\n",
    "    \"\"\"\n",
    "    Function to process hourly data into daily averages.\n",
    "    Args:\n",
    "        data (list): A list of hourly measurements.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with daily averages of the measurements.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"])\n",
    "    df = df.rename(columns={\"Warto≈õƒá\": \"value\"})\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    df[\"date\"] = df[\"Data\"].dt.date\n",
    "    daily_avg = df.groupby(\"date\")[\"value\"].mean().reset_index()\n",
    "    return daily_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afcbba",
   "metadata": {},
   "source": [
    "## Data extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = get_sensors(station_id)  # Download sensors for the station\n",
    "\n",
    "pm25_sensors = [s for s in sensors if s.get(\"Wska≈∫nik - wz√≥r\") == \"PM2.5\"]\n",
    "pm10_sensors = [s for s in sensors if s.get(\"Wska≈∫nik - wz√≥r\") == \"PM10\"]\n",
    "\n",
    "result_dfs = []\n",
    "\n",
    "def process_sensor_group(sensor_group, param_formula):\n",
    "    \"\"\"\n",
    "    Function to process a group of sensors for a specific parameter (e.g., PM2.5 or PM10).\n",
    "    Args:\n",
    "        sensor_group (list): A list of sensors to process.\n",
    "        param_formula (str): The parameter formula to use for renaming the DataFrame.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with daily averages for the specified parameter.\n",
    "    \"\"\"\n",
    "    group_dfs = []\n",
    "\n",
    "    for sensor in sensor_group:\n",
    "        sensor_id = sensor.get(\"Identyfikator stanowiska\")\n",
    "        print(f\"üîç Sensor: {param_formula} (ID: {sensor_id})\")\n",
    "\n",
    "        current_start = start_date\n",
    "        all_records = []\n",
    "\n",
    "        while current_start <= end_date:\n",
    "            segment_end = min(current_start + timedelta(days=365), end_date)\n",
    "            date_from_str = current_start.strftime(\"%Y-%m-%d %H:%M\")\n",
    "            date_to_str = segment_end.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "            print(f\"Timestamp: {date_from_str} ‚Üí {date_to_str}\")\n",
    "            data = get_archival_data(sensor_id, date_from_str, date_to_str)\n",
    "            all_records.extend(data)\n",
    "\n",
    "            current_start = segment_end + timedelta(days=1)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        daily_df = process_hourly_to_daily(all_records)\n",
    "\n",
    "        if not daily_df.empty:\n",
    "            daily_df.rename(columns={\"value\": param_formula}, inplace=True)\n",
    "            group_dfs.append(daily_df.set_index(\"date\"))\n",
    "\n",
    "    # Merge all dataframes for this group by date (average if multiple)\n",
    "    if group_dfs:\n",
    "        combined = pd.concat(group_dfs, axis=1)\n",
    "        combined = combined.groupby(combined.index).mean()  # average if multiple sensors\n",
    "        return combined\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Process PM2.5 and PM10\n",
    "pm25_df = process_sensor_group(pm25_sensors, \"PM2.5\")\n",
    "pm10_df = process_sensor_group(pm10_sensors, \"PM10\")\n",
    "\n",
    "# Combine both into one DataFrame\n",
    "final_df = pd.concat([pm25_df, pm10_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca72e9",
   "metadata": {},
   "source": [
    "## Saving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98545c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_df.empty:\n",
    "    final_df.sort_index(inplace=True)\n",
    "    filename = \"pomiar2025.csv\"\n",
    "    final_df.to_csv(filename)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "else:\n",
    "    print(\"No data available for the specified date range.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
